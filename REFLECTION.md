## Reflection

Using AI to generate unit and integration tests for this project was both helpful and eye-opening. The generated tests covered a wide range of scenarios, including normal cases, errors, and edge conditions such as invalid tokens or very large IDs. However, when I ran the tests locally, I noticed that several of them failed because the expected outputs did not fully match my current implementation. For example, many tests expected a 200, 500, or 401 status, while my API consistently returned 404 for those routes. This discrepancy revealed that the AI created tests based on assumptions of a “typical” user service, not necessarily how my simplified version behaves.

Rather than rewriting the entire application to satisfy every test, I used this as an opportunity to analyze the mismatches and better understand how my code handles errors. The integration tests passed, confirming that the overall wiring works. In the end, the AI was useful for surfacing potential gaps and encouraging me to think critically about how my API should behave in real-world scenarios.